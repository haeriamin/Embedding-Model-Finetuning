{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Update venv\\Lib\\site-packages\\llama_index\\finetuning\\embeddings\\common.py\n",
    "def generate_qa_embedding_pairs\n",
    "    ...\n",
    "    save_counter = start_index\n",
    "\n",
    "    # added --------------------------------------------------------------\n",
    "    import time\n",
    "    counter, start_time = 0, time.time()\n",
    "    # --------------------------------------------------------------------\n",
    "\n",
    "    for node_id, text in tqdm(\n",
    "        list(node_dict.items())[start_index:], initial=start_index\n",
    "    ):\n",
    "        \n",
    "        # added --------------------------------------------------------------\n",
    "        counter += 1\n",
    "        if counter > 10 and time.time() - start_time < 60:\n",
    "            time.sleep(60 - (time.time() - start_time))\n",
    "            counter, start_time = 0, time.time()\n",
    "        # --------------------------------------------------------------------\n",
    "\n",
    "        query = qa_generate_prompt_tmpl.format(\n",
    "            context_str=text, num_questions_per_chunk=num_questions_per_chunk\n",
    "        )\n",
    "    ...\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import openparse\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "import google.generativeai as genai\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from transformers import BitsAndBytesConfig\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.finetuning import generate_qa_embedding_pairs\n",
    "from llama_index.core.llms.chatml_utils import messages_to_prompt, completion_to_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_CACHE_DIR = \"../models/hf\"\n",
    "os.environ['HF_HOME'] = HF_CACHE_DIR\n",
    "\n",
    "TIKTOKEN_CACHE_DIR = \"../models/tiktoken\"\n",
    "os.environ[\"TIKTOKEN_CACHE_DIR\"] = TIKTOKEN_CACHE_DIR\n",
    "# assert os.path.exists(os.path.join(TIKTOKEN_CACHE_DIR, \"9b5ad71b2ce5302211f9c61530b329a4922fc6a4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read API tokens (SHOULD BE CREATED BY USER)\n",
    "with open('../reqs/tokens.json', 'r') as file:\n",
    "    tokens = json.load(file)\n",
    "\n",
    "HF_ACCESS_TOKEN = tokens['HF_ACCESS_TOKEN'][0]\n",
    "GOOGLE_API_KEY = tokens['GOOGLE_API_KEY'][0]\n",
    "OPENAI_API_KEY = tokens['OPENAI_API_KEY'][0]\n",
    "\n",
    "login(token=HF_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set/Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API model\n",
    "llm = Gemini(\n",
    "    api_key = GOOGLE_API_KEY,\n",
    "    model = \"models/gemini-1.0-pro\",\n",
    "    temperature = 0.3,\n",
    ")\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Local model\n",
    "# llm_name = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit = True,\n",
    "#     bnb_4bit_compute_dtype = torch.float16,\n",
    "#     bnb_4bit_quant_type = \"nf4\",\n",
    "#     bnb_4bit_use_double_quant = True,\n",
    "# )\n",
    "\n",
    "# llm = HuggingFaceLLM(\n",
    "#     model_name = llm_name,\n",
    "#     tokenizer_name = llm_name,\n",
    "#     context_window = 2048,\n",
    "#     max_new_tokens = 512,\n",
    "\n",
    "#     generate_kwargs = {\n",
    "#         \"do_sample\": True,\n",
    "#         \"temperature\": 0.5,\n",
    "#     },\n",
    "#     model_kwargs = {\n",
    "#         # \"torch_dtype\": torch.float16,\n",
    "#         \"quantization_config\": quantization_config,\n",
    "#         \"cache_dir\": HF_CACHE_DIR,\n",
    "#     },\n",
    "#     device_map = \"auto\",\n",
    "#     is_chat_model = True,\n",
    "\n",
    "#     completion_to_prompt = completion_to_prompt,\n",
    "#     messages_to_prompt = messages_to_prompt,\n",
    "# )\n",
    "\n",
    "# Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_folders = 5\n",
    "train_files = []\n",
    "\n",
    "for i in range(num_train_folders):\n",
    "    train_files.append(glob.glob(f\"../data/finetune/docs/train_{i+1}/*.pdf\"))\n",
    "\n",
    "val_files = glob.glob(\"../data/finetune/docs/val/*.pdf\")\n",
    "\n",
    "train_files, val_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_corpus(files):\n",
    "    parser = openparse.DocumentParser(\n",
    "        table_args = {\"parsing_algorithm\": \"pymupdf\",},\n",
    "    )\n",
    "\n",
    "    nodes = []\n",
    "    for file in tqdm(files):\n",
    "        try:\n",
    "            nodes += parser.parse(file, ocr=True).to_llama_index_nodes()\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return nodes\n",
    "\n",
    "# Parse and save\n",
    "for i in range(num_train_folders):\n",
    "    train_nodes = parse_corpus(train_files[i])\n",
    "    with open(f'../data/finetune/docs/train_{i+1}/nodes.pkl', 'wb') as file: pickle.dump(train_nodes, file)\n",
    "    print(len(train_nodes))\n",
    "    \n",
    "val_nodes = parse_corpus(val_files)\n",
    "with open('../data/finetune/docs/val/nodes.pkl', 'wb') as file: pickle.dump(val_nodes, file) \n",
    "print(len(val_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_train_folders):\n",
    "    if i + 1 == 1:\n",
    "        with open(f'../data/finetune/docs/train_{i+1}/nodes.pkl', 'rb') as file:\n",
    "            train_nodes = pickle.load(file)\n",
    "\n",
    "        train_dataset = generate_qa_embedding_pairs(\n",
    "            llm = llm,\n",
    "            nodes = train_nodes,\n",
    "            num_questions_per_chunk = 2,\n",
    "            output_path = f\"../data/finetune/datasets/train_{i+1}.json\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = generate_qa_embedding_pairs(\n",
    "    llm = llm,\n",
    "    nodes = val_nodes,\n",
    "    num_questions_per_chunk = 2,\n",
    "    output_path = f\"../data/finetune/datasets/val.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Dataset (Part I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.replace('*', ' ').split(' ')\n",
    "    text_modified = ''\n",
    "    for t in text:\n",
    "        if not t.isupper():\n",
    "            t = t.lower()\n",
    "        text_modified += t + ' '\n",
    "    return text_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_queries = (\"**Question 1:**\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set\n",
    "# Combine train sets\n",
    "json_files = glob.glob(\"../data/finetune/datasets/train_*.json\")\n",
    "train_sets = []\n",
    "\n",
    "for f in json_files:\n",
    "    with open(f, 'r') as file:\n",
    "        train_sets.append(json.load(file))\n",
    "\n",
    "queries_size = []\n",
    "corpus_size = []\n",
    "relevant_docs_size = []\n",
    "\n",
    "for train_set in train_sets:\n",
    "    queries_size.append(len(train_set['queries'].keys()))\n",
    "    corpus_size.append(len(train_set['corpus'].keys()))\n",
    "    relevant_docs_size.append(len(train_set['relevant_docs'].keys()))\n",
    "\n",
    "train_set = {}\n",
    "for t in train_sets:\n",
    "    for key1 in t:\n",
    "        for key2 in t[key1]:\n",
    "            if key1 == 'mode':\n",
    "                continue\n",
    "            if key1 not in train_set.keys():\n",
    "                train_set[key1] = {}\n",
    "            train_set[key1][key2] = t[key1][key2]\n",
    "\n",
    "train_set[\"mode\"] = \"text\"\n",
    "\n",
    "assert sum(queries_size) == len(train_set['queries'].keys()), \"Unmatched number of queries\"\n",
    "assert sum(corpus_size) == len(train_set['corpus'].keys()), \"Unmatched number of corpus\"\n",
    "assert sum(relevant_docs_size) == len(train_set['relevant_docs'].keys()), \"Unmatched number of relevant_docs\"\n",
    "\n",
    "# Remove bad questions\n",
    "train_set['queries'] = {key:val for key, val in train_set['queries'].items() if val not in bad_queries}\n",
    "train_set['queries'] = {key:val for key, val in train_set['queries'].items() if 'question 1' not in val.lower()}\n",
    "\n",
    "# Process\n",
    "for key, val in train_set['queries'].items():\n",
    "    val = preprocess(val)\n",
    "    train_set['queries'][key] = val\n",
    "for key, val in train_set['corpus'].items():\n",
    "    val = preprocess(val)\n",
    "    train_set['corpus'][key] = val\n",
    "\n",
    "# Save\n",
    "with open(\"../data/finetune/datasets/train.json\", 'w') as f:\n",
    "    json.dump(train_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set\n",
    "with open(\"../data/finetune/datasets/val.json\", 'r') as file:\n",
    "    val_dataset = json.load(file)\n",
    "\n",
    "# Remove bad questions\n",
    "val_dataset['queries'] = {key:val for key, val in val_dataset['queries'].items() if val not in bad_queries}\n",
    "val_dataset['queries'] = {key:val for key, val in val_dataset['queries'].items() if 'question 1' not in val.lower()}\n",
    "\n",
    "# Process\n",
    "for key, val in val_dataset['queries'].items():\n",
    "    val = preprocess(val)\n",
    "    val_dataset['queries'][key] = val\n",
    "for key, val in val_dataset['corpus'].items():\n",
    "    val = preprocess(val)\n",
    "    val_dataset['corpus'][key] = val\n",
    "\n",
    "# Save\n",
    "with open(\"../data/finetune/datasets/val.json\", 'w') as f:\n",
    "    json.dump(val_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(train_set['queries'].keys())) + len(list(val_dataset['queries'].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Dataset (Part II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_2(text):\n",
    "    bad_strings = (\n",
    "        '<br><br>', '<th></th>', '<td></td>', '<table border=\\\"1\\\">',\n",
    "        '<tr></tr>', '<table></table>', '<td>', '</td>', '<tr>', '</tr>', '<table>', '</table>', '<th>', '</th>', '<br>',\n",
    "        '<p>', '</p>', '<h1>', '</h1>', '<h2>', '</h2>', '<h3>', '</h3>', '<h4>', '</h4>', '<h5>', '</h5>', '<h6>', '</h6>', '<ul>', '</ul>', '<ol>', '</ol>', '<li>', '</li>',\n",
    "        '<strong>', '</strong>', '<em>', '</em>', '<b>', '</b>', '<i>', '</i>', '<u>', '</u>', '<sub>', '</sub>', '<sup>', '</sup>', '<code>', '</code>', '<pre>', '</pre>', '<blockquote>', '</blockquote>', '<hr>', '<br>', '<br />', '<br/>',\n",
    "    )\n",
    "    for bad_string in bad_strings:\n",
    "        text = text.replace(bad_string, '\\n')\n",
    "\n",
    "    bad_string = '\\n'\n",
    "    for _ in range(10):\n",
    "        bad_string += '\\n'\n",
    "        text = text.replace(bad_string, '\\n')\n",
    "    \n",
    "    bad_string = '\\n'\n",
    "    for _ in range(10):\n",
    "        bad_string += ' \\n'\n",
    "        text = text.replace(bad_string, '\\n')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def convert_to_question_context_pairs(dataset):\n",
    "    question_context = {}\n",
    "\n",
    "    counter = 0\n",
    "    for key in dataset['queries'].keys():\n",
    "        counter += 1\n",
    "        question = dataset['queries'][key]\n",
    "        val = dataset['relevant_docs'][key][0]\n",
    "        context = dataset['corpus'][val]\n",
    "        context = preprocess_2(context)\n",
    "        question_context[counter] = [key, question, context]\n",
    "\n",
    "    return question_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set\n",
    "with open(\"../data/finetune/datasets/train.json\", 'r') as file:\n",
    "    train_dataset = json.load(file)\n",
    "\n",
    "train_pairs = convert_to_question_context_pairs(train_dataset)\n",
    "with open(\"../data/finetune/datasets/test/train_pairs.json\", 'w') as f:\n",
    "    json.dump(train_pairs, f)\n",
    "\n",
    "# # Check percentage of removed items\n",
    "# with open(\"../data/finetune/datasets/test/modified_train_pairs.json\", 'r') as file:\n",
    "#     modified_train_pairs = json.load(file)\n",
    "# round((len(list(train_pairs.keys())) - len(list(modified_train_pairs.keys()))) / len(list(train_pairs.keys())) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation set\n",
    "with open(\"../data/finetune/datasets/val.json\", 'r') as file:\n",
    "    val_dataset = json.load(file)\n",
    "\n",
    "val_pairs = convert_to_question_context_pairs(val_dataset)\n",
    "with open(\"../data/finetune/datasets/test/val_pairs.json\", 'w') as f:\n",
    "    json.dump(val_pairs, f)\n",
    "\n",
    "# Check percentage of removed items\n",
    "with open(\"../data/finetune/datasets/test/modified_validation_pairs.json\", 'r') as file:\n",
    "    modified_validation_pairs = json.load(file)\n",
    "round((len(list(val_pairs.keys())) - len(list(modified_validation_pairs.keys()))) / len(list(val_pairs.keys())) * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
