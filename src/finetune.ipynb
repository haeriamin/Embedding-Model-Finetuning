{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# For models that have a rate limit of 1 per second (like Mistral)\n",
    "# Update venv\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py\n",
    "    def _add_nodes_to_index(\n",
    "        self,\n",
    "        index_struct: IndexDict,\n",
    "        nodes: Sequence[BaseNode],\n",
    "        show_progress: bool = False,\n",
    "        **insert_kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Add document to index.\"\"\"\n",
    "        if not nodes:\n",
    "            return\n",
    "\n",
    "\n",
    "        # added --------------------------------------------------------------\n",
    "        import time\n",
    "        # --------------------------------------------------------------------\n",
    "        for nodes_batch in iter_batch(nodes, self._insert_batch_size):\n",
    "            # added (for Mistral) ------------------------------------------------\n",
    "            # time.sleep(2)\n",
    "            # --------------------------------------------------------------------\n",
    "            # added (for Voayage) ------------------------------------------------\n",
    "            # time.sleep(60)\n",
    "            # --------------------------------------------------------------------\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer, losses\n",
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "from llama_index.finetuning import SentenceTransformersFinetuneEngine\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding, OpenAIEmbeddingModelType\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
    "from llama_index.embeddings.voyageai import VoyageEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.schema import TextNode\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stransformers_cache_dir = \"../models/stransformers\"\n",
    "os.environ[\"SENTENCE_TRANSFORMERS_HOME\"] = stransformers_cache_dir\n",
    "\n",
    "HF_CACHE_DIR = \"../models/hf\"\n",
    "os.environ['HF_HOME'] = HF_CACHE_DIR\n",
    "\n",
    "TIKTOKEN_CACHE_DIR = \"../models/tiktoken\"\n",
    "os.environ[\"TIKTOKEN_CACHE_DIR\"] = TIKTOKEN_CACHE_DIR\n",
    "\n",
    "with open('../reqs/tokens.json', 'r') as file:\n",
    "    tokens = json.load(file)\n",
    "\n",
    "GOOGLE_API_KEY = tokens['GOOGLE_API_KEY'][0]\n",
    "OPENAI_API_KEY = tokens['OPENAI_API_KEY'][0]\n",
    "COHERE_API_KEY = tokens['COHERE_API_KEY'][0]\n",
    "MISTRALAI_API_KEY = tokens['MISTRALAI_API_KEY'][0]\n",
    "VOYAGEAI_API_KEY = tokens['VOYAGEAI_API_KEY'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmbeddingQAFinetuneDataset.from_json(\"../data/finetune/datasets/train.json\")\n",
    "val_dataset = EmbeddingQAFinetuneDataset.from_json(\"../data/finetune/datasets/val.json\")\n",
    "models = [\n",
    "    \"Snowflake/snowflake-arctic-embed-m\",\n",
    "    \"Snowflake/snowflake-arctic-embed-l\",\n",
    "    \"dunzhang/stella_en_400M_v5l\",\n",
    "]\n",
    "\n",
    "model_index = 1\n",
    "start = 3\n",
    "num_times = 3\n",
    "epochs = 2\n",
    "# max_epoch = num_times * epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset.corpus.keys()), len(val_dataset.corpus.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gradually fine-tune models while increasing number of epochs\n",
    "# for i in range(start, num_times + 1):\n",
    "#     model_id = \\\n",
    "#         models[model_index-1] if i == 1 else \\\n",
    "#         f\"../models/stransformers/{models[model_index-1].split('/')[-1]}-token-finetuned-{(i-1)*epochs}\"\n",
    "\n",
    "#     # model = SentenceTransformer(embedding_name, trust_remote_code=True)\n",
    "#     # loss = losses.MultipleNegativesRankingLoss(model)\n",
    "#     # loss = losses.MatryoshkaLoss(model, loss, [768, 256])  # for m\n",
    "#     # loss = losses.MatryoshkaLoss(model, loss, [1024, 512])  # for l\n",
    "\n",
    "#     finetune_engine = SentenceTransformersFinetuneEngine(\n",
    "#         dataset = train_dataset,\n",
    "#         model_id = model_id,\n",
    "#         model_output_path = \\\n",
    "#             f\"../models/stransformers/{model_id.split('/')[-1]}-token-finetuned-{i*epochs}\" if i == 1 else \\\n",
    "#             f\"../models/stransformers/{model_id.split('/')[-1].replace(str((i-1)*epochs), str(i*epochs))}\",\n",
    "#         val_dataset = val_dataset,\n",
    "#         epochs = epochs,\n",
    "#         trust_remote_code = True,\n",
    "#         batch_size = 12 if model_index == 1 else 2,\n",
    "#         evaluation_steps = 100 if model_index == 1 else 200,\n",
    "#         # loss = loss,\n",
    "#     )\n",
    "\n",
    "#     finetune_engine.finetune()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_for_all_models(\n",
    "        embed_model, dataset, output_path,\n",
    "        batch_size, show_progress, sleep, rate_limit,\n",
    "        top_k=5, local=False,\n",
    "    ):\n",
    "    Path(output_path).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    corpus = dataset.corpus\n",
    "    queries = dataset.queries\n",
    "    relevant_docs = dataset.relevant_docs\n",
    "\n",
    "    nodes = [TextNode(id_=id_, text=text) for id_, text in corpus.items()]\n",
    "    if local:\n",
    "        embed_model = HuggingFaceEmbedding(model_name=embed_model)\n",
    "\n",
    "    index = VectorStoreIndex(\n",
    "        nodes,\n",
    "        embed_model=embed_model,\n",
    "        show_progress=show_progress,\n",
    "        insert_batch_size=batch_size,\n",
    "    )\n",
    "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
    "\n",
    "    eval_results, counter = [], 0\n",
    "    for query_id, query in tqdm(queries.items()):\n",
    "        time.sleep(sleep)\n",
    "        counter += 1\n",
    "        if counter == rate_limit:\n",
    "            time.sleep(60)\n",
    "            counter = 0\n",
    "        retrieved_nodes = retriever.retrieve(query)\n",
    "        retrieved_ids = [node.node.node_id for node in retrieved_nodes]\n",
    "        expected_id = relevant_docs[query_id][0]\n",
    "        is_hit = expected_id in retrieved_ids  # assume 1 relevant doc\n",
    "\n",
    "        eval_result = {\n",
    "            \"is_hit\": is_hit,\n",
    "            \"retrieved\": retrieved_ids,\n",
    "            \"expected\": expected_id,\n",
    "            \"query\": query_id,\n",
    "        }\n",
    "        eval_results.append(eval_result)\n",
    "\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_for_st_models(model_id, dataset, name, output_path):\n",
    "    Path(output_path).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    corpus = dataset.corpus\n",
    "    queries = dataset.queries\n",
    "    relevant_docs = dataset.relevant_docs\n",
    "    evaluator = InformationRetrievalEvaluator(\n",
    "        queries, corpus, relevant_docs, name=name,\n",
    "    )\n",
    "\n",
    "    model = SentenceTransformer(model_id)\n",
    "\n",
    "    return evaluator(model, output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_based_results(\n",
    "        company, embedding_name, embedding, dataset, results,\n",
    "        batch_size, show_progress, sleep, rate_limit, load):\n",
    "    if not load:\n",
    "        output_path = f\"../results/{company}-{embedding_name}\"\n",
    "        results[f'{company}-{embedding_name}'] = evaluate_for_all_models(\n",
    "            embedding, dataset, output_path,\n",
    "            batch_size, show_progress, sleep, rate_limit,\n",
    "        )\n",
    "\n",
    "        df = pd.DataFrame(results[f'{company}-{embedding_name}'])\n",
    "        df.to_csv(f\"../results/{company}-{embedding_name[:]}/hit_rates.csv\", index=False)\n",
    "    \n",
    "    else:\n",
    "        df = pd.read_csv(f\"../results/{company}-{embedding_name}/hit_rates.csv\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results_st = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = True\n",
    "\n",
    "rate_limit = 1000  # per min\n",
    "sleep = 0\n",
    "show_progress = True\n",
    "batch_size = None\n",
    "\n",
    "company = 'google'\n",
    "embedding_name = 'text-embedding-004'\n",
    "embedding = GeminiEmbedding(\n",
    "    api_key = GOOGLE_API_KEY,\n",
    "    model_name = f\"models/{embedding_name}\",\n",
    ")\n",
    "\n",
    "results[f'{company}-{embedding_name}'] = get_api_based_results(\n",
    "    company, embedding_name, embedding,\n",
    "    val_dataset, results,\n",
    "    batch_size, show_progress, sleep, rate_limit,\n",
    "    load,\n",
    ")\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load = False\n",
    "\n",
    "# rate_limit = 9  # per min\n",
    "# sleep = 60\n",
    "# show_progress = True\n",
    "# batch_size = 9\n",
    "\n",
    "# company = 'google'\n",
    "# embedding_name = 'gemini-embedding-exp-03-07'\n",
    "# embedding = GeminiEmbedding(\n",
    "#     api_key = GOOGLE_API_KEY,\n",
    "#     model_name = f\"models/{embedding_name}\",\n",
    "# )\n",
    "\n",
    "# results[f'{company}-{embedding_name}'] = get_api_based_results(\n",
    "#     company, embedding_name, embedding,\n",
    "#     val_dataset, results,\n",
    "#     batch_size, show_progress, sleep, rate_limit,\n",
    "#     load,\n",
    "# )\n",
    "\n",
    "# embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = True\n",
    "\n",
    "rate_limit = 50  # per min\n",
    "sleep = 0\n",
    "show_progress = True\n",
    "batch_size = None\n",
    "\n",
    "company = 'openai'\n",
    "# embedding_name = OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002\n",
    "embedding_name = OpenAIEmbeddingModelType.TEXT_EMBED_3_LARGE\n",
    "embedding = OpenAIEmbedding(\n",
    "    api_key = OPENAI_API_KEY,\n",
    "    model = embedding_name,\n",
    ")\n",
    "\n",
    "results[f'{company}-{embedding_name}'] = get_api_based_results(\n",
    "    company, embedding_name, embedding,\n",
    "    val_dataset, results,\n",
    "    batch_size, show_progress, sleep, rate_limit,\n",
    "    load,\n",
    ")\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohere's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = True\n",
    "\n",
    "rate_limit = 50  # per min\n",
    "sleep = 0\n",
    "show_progress = True\n",
    "batch_size = None\n",
    "\n",
    "company = 'cohere'\n",
    "embedding_name = \"embed-english-v3.0\"\n",
    "embedding = CohereEmbedding(\n",
    "    api_key = COHERE_API_KEY,\n",
    "    model = embedding_name,\n",
    "    input_type=\"search_query\",\n",
    ")\n",
    "\n",
    "results[f'{company}-{embedding_name}'] = get_api_based_results(\n",
    "    company, embedding_name, embedding,\n",
    "    val_dataset, results,\n",
    "    batch_size, show_progress, sleep, rate_limit,\n",
    "    load,\n",
    ")\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = True\n",
    "\n",
    "rate_limit = 1000  # per min\n",
    "sleep = 2\n",
    "show_progress = False\n",
    "batch_size = 1\n",
    "\n",
    "company = 'mistralai'\n",
    "embedding_name = \"mistral-embed\"\n",
    "embedding = MistralAIEmbedding(\n",
    "    api_key = MISTRALAI_API_KEY,\n",
    "    model = embedding_name,\n",
    "    input_type=\"search_query\",\n",
    ")\n",
    "\n",
    "results[f'{company}-{embedding_name}'] = get_api_based_results(\n",
    "    company, embedding_name, embedding,\n",
    "    val_dataset, results,\n",
    "    batch_size, show_progress, sleep, rate_limit,\n",
    "    load,\n",
    ")\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VoyageAI's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = True\n",
    "\n",
    "rate_limit = 3  # per min\n",
    "sleep = 0\n",
    "show_progress = True\n",
    "batch_size = 3\n",
    "\n",
    "company = 'voyageai'\n",
    "# embedding_name = \"voyage-3-large\"\n",
    "embedding_name = \"voyage-finance-2\"\n",
    "embedding = VoyageEmbedding(\n",
    "    voyage_api_key = VOYAGEAI_API_KEY,\n",
    "    model_name = embedding_name,\n",
    ")\n",
    "\n",
    "results[f'{company}-{embedding_name}'] = get_api_based_results(\n",
    "    company, embedding_name, embedding,\n",
    "    val_dataset, results,\n",
    "    batch_size, show_progress, sleep, rate_limit,\n",
    "    load,\n",
    ")\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence transformers comprehensive evaluation\n",
    "load = True\n",
    "st = False\n",
    "\n",
    "rate_limit = 1e10  # per min\n",
    "sleep = 0\n",
    "show_progress = False\n",
    "batch_size = None\n",
    "local = True\n",
    "\n",
    "for i in range(1, num_times + 2):\n",
    "    try:\n",
    "        model_base = models[model_index-1]\n",
    "        if i == 1:\n",
    "            # Base\n",
    "            output_path = f\"../results/{model_base.split('/')[-1]}\"\n",
    "\n",
    "            if not load:\n",
    "                embed_model = f\"../models/stransformers/{model_base.split('/')[-1]}\"\n",
    "                results['base'] = pd.DataFrame(evaluate_for_all_models(\n",
    "                    embed_model, val_dataset, output_path, batch_size, show_progress, sleep, rate_limit, local\n",
    "                ))\n",
    "                results['base'].to_csv(f\"{output_path}/hit_rates.csv\", index=False)\n",
    "            else:\n",
    "                results['base'] = pd.read_csv(f\"{output_path}/hit_rates.csv\")\n",
    "\n",
    "            if st:\n",
    "                results_st['base'] = evaluate_for_st_models(model_base, val_dataset, \"base\", output_path)\n",
    "        else:\n",
    "\n",
    "            # Finetuned\n",
    "            output_path = f\"../results/{model_base.split('/')[-1]}-finetuned-2\"\n",
    "            model_finetuned = f\"../models/stransformers/{model_base.split('/')[-1]}-finetuned-2\"\n",
    "\n",
    "            if not load:\n",
    "                embed_model = f\"../models/stransformers/{model_base.split('/')[-1]}-finetuned-2\"\n",
    "                results['finetuned-2'] = pd.DataFrame(evaluate_for_all_models(\n",
    "                    embed_model, val_dataset, output_path, batch_size, show_progress, sleep, rate_limit, local\n",
    "                ))\n",
    "                results['finetuned-2'].to_csv(f\"{output_path}/hit_rates.csv\", index=False)\n",
    "            else:\n",
    "                results['finetuned-2'] = pd.read_csv(f\"{output_path}/hit_rates.csv\")\n",
    "            \n",
    "            if st:\n",
    "                results_st[f\"finetuned-2\"] = evaluate_for_st_models(model_finetuned, val_dataset, f\"finetuned-2\", output_path)\n",
    "\n",
    "            # Token finetuned\n",
    "            output_path = f\"../results/{model_base.split('/')[-1]}-token-finetuned-{(i-1)*epochs}\"\n",
    "            model_token_finetuned = f\"../models/stransformers/{models[model_index-1].split('/')[-1]}-token-finetuned-{(i-1)*epochs}\"\n",
    "\n",
    "            if not load:\n",
    "                embed_model = f\"../models/stransformers/{model_base.split('/')[-1]}-token-finetuned-{(i-1)*epochs}\"\n",
    "                results[f'token-finetuned-{(i-1)*epochs}'] = pd.DataFrame(evaluate_for_all_models(\n",
    "                    embed_model, val_dataset, output_path, batch_size, show_progress, sleep, rate_limit, local\n",
    "                ))\n",
    "                results[f'token-finetuned-{(i-1)*epochs}'].to_csv(f\"{output_path}/hit_rates.csv\", index=False)\n",
    "            else:\n",
    "                results[f'token-finetuned-{(i-1)*epochs}'] = pd.read_csv(f\"{output_path}/hit_rates.csv\")\n",
    "\n",
    "            if st:\n",
    "                results_st[f\"token-finetuned-{(i-1)*epochs}\"] = evaluate_for_st_models(model_token_finetuned, val_dataset, f\"token-finetuned-{(i-1)*epochs}\", output_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hit Rate @ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of parameters, and Embedding dimension\n",
    "model_size = {\n",
    "    'google-text-embedding-004': ['X', '768'],\n",
    "    'google-gemini-embedding-exp-03-07': ['X', '3072'],\n",
    "    'openai-text-embedding-3-large': ['X', '3072'],\n",
    "    'openai-text-embedding-3-small': ['X', '1536'], # better than ada\n",
    "    'openai-text-embedding-ada-002': ['X', '1536'],\n",
    "    'cohere-embed-english-v3.0': ['X', '1024'],\n",
    "    'mistralai-mistral-embed': ['X', '1024'],\n",
    "    'voyageai-voyage-finance-2': ['X', '1024'],\n",
    "    'base': ['305M', '768'],\n",
    "    'finetuned-2': ['305M', '768'],\n",
    "    'token-finetuned-2': ['305M+', '768'],\n",
    "    'token-finetuned-4': ['305M+', '768'],\n",
    "    'token-finetuned-6': ['305M+', '768'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hit Rate @ 5 Comparison:')\n",
    "for key in results.keys():\n",
    "    df = results[key]\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        df = pd.DataFrame(df)\n",
    "    try:\n",
    "        if key == 'base':\n",
    "            _key = models[model_index-1].split('/')[1] + ' (base)'\n",
    "            print(f' {_key:31}: {round(df[\"is_hit\"].mean()*100, 1):4}%  (with model size of {model_size[key][0]:5} and embedding size of {model_size[key][1]:4})')\n",
    "        else:\n",
    "            print(f' {key:31}: {round(df[\"is_hit\"].mean()*100, 1):4}%  (with model size of {model_size[key][0]:5} and embedding size of {model_size[key][1]:4})')\n",
    "    except:\n",
    "        print(f' {key:31}: {round(df[\"is_hit\"].mean()*100, 1):4}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in results_st.keys():\n",
    "    res = results_st[key]\n",
    "    improvement = round((res - results_st['base']) / results_st['base'] * 100)\n",
    "    print(f'{key}: {round(res*100, 1)}% (with {improvement}% improvement)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "colors = px.colors.sequential.Turbo\n",
    "# colors = px.colors.qualitative.G10\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Metric\": [\"MRR@10\", \"MAP@100\", \"NDCG@10\"],\n",
    "    \"Base\": [38, 39, 43],\n",
    "    \"Finetuned\": [84, 84, 86]\n",
    "})\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Bar(\n",
    "            name='Base',\n",
    "            x=df['Metric'],\n",
    "            y=df['Base'],\n",
    "            marker=dict(\n",
    "                color='rgba(255, 255, 255, 0)',  # Transparent fill color\n",
    "                line=dict(color=colors[-3], width=10)  # Black border around bars\n",
    "            ),\n",
    "            text=[str(i)+'%' for i in df['Base'].values],\n",
    "            textposition='auto',\n",
    "        ),\n",
    "        go.Bar(\n",
    "            name='Finetuned',\n",
    "            x=df['Metric'],\n",
    "            y=df['Finetuned'],\n",
    "            marker=dict(\n",
    "                color='rgba(255, 255, 255, 0)',  # Transparent fill color\n",
    "                line=dict(color=colors[1], width=10)  # Black border around bars\n",
    "            ),\n",
    "            text=[str(i)+'%' for i in df['Finetuned'].values],\n",
    "            textposition='auto',\n",
    "        ),\n",
    "    ],\n",
    "    layout=dict(\n",
    "        barcornerradius=15,\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=600, width=1400,\n",
    "    # title='Performance Comparison',\n",
    "    xaxis_title='Metric',\n",
    "    template=\"plotly_white\", # use white background\n",
    "    yaxis=dict(range=[-5, 100]),\n",
    "    font=dict(size=18),\n",
    "    barmode='group',\n",
    "    bargap=0.2, # gap between bars of adjacent location coordinates.\n",
    "    bargroupgap=0.1 # gap between bars of the same location coordinate.\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_image('../results/perf.png', scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Model': ['Google Text-Embedding-004', 'Cohere Embed-English-v3.0', 'OpenAI Text-Embedding-3-Large', 'MistralAI Mistral-Embed', 'Voyage AI Voyage-Finance-2', 'Ours'],\n",
    "    'HR@5 [%]': [84, 85, 86, 87, 88, 88],\n",
    "    'Improvement [%]': [5, 4, 2, 1, 0, None],\n",
    "    'Embedding Size': [768, 1024, 3072, 1024, 1024, 768]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df['Improvement [%]'] = df['Improvement [%]'].fillna(0)\n",
    "\n",
    "# Create a bubble chart with color based on Improvement\n",
    "fig_bubble_color = go.Figure(data=go.Scatter(\n",
    "    x=df['Model'],\n",
    "    y=df['HR@5 [%]'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        symbol=\"octagon-open-dot\",\n",
    "        size=df['Embedding Size'] / 15,  # Scale the size\n",
    "        sizemode='diameter',\n",
    "        color=df['Improvement [%]'],  # Color based on Improvement\n",
    "        # colorscale='turbo',\n",
    "        colorscale=px.colors.sequential.Turbo[1:4] + px.colors.sequential.Turbo[-4:-1],\n",
    "        colorbar=dict(title='Imp.'),  # Add a colorbar\n",
    "        opacity=1,\n",
    "        line_width=10,\n",
    "    ),\n",
    "    text=df['Model']\n",
    "))\n",
    "\n",
    "fig_bubble_color.update_layout(\n",
    "    height=600, width=1400,\n",
    "    # title='Performance Benchmarking',\n",
    "    # xaxis_title='Model',\n",
    "    yaxis_title='HR@5 [%]',\n",
    "    template=\"plotly_white\", # use white background\n",
    "    yaxis=dict(range=[79, 90]), # set the y-axis range\n",
    "    font=dict(size=18) # Increase font size\n",
    ")\n",
    "\n",
    "fig_bubble_color.show()\n",
    "fig_bubble_color.write_image('../results/bench.png', scale=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
