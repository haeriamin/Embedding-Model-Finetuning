{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Update venv\\Lib\\site-packages\\llama_index\\finetuning\\embeddings\\common.py\n",
    "def generate_qa_embedding_pairs\n",
    "    ...\n",
    "    save_counter = start_index\n",
    "\n",
    "    # added --------------------------------------------------------------\n",
    "    import time\n",
    "    counter, start_time = 0, time.time()\n",
    "    # --------------------------------------------------------------------\n",
    "\n",
    "    for node_id, text in tqdm(\n",
    "        list(node_dict.items())[start_index:], initial=start_index\n",
    "    ):\n",
    "        \n",
    "        # added --------------------------------------------------------------\n",
    "        counter += 1\n",
    "        if counter > 10 and time.time() - start_time < 60:\n",
    "            time.sleep(60 - (time.time() - start_time))\n",
    "            counter, start_time = 0, time.time()\n",
    "        # --------------------------------------------------------------------\n",
    "\n",
    "        query = qa_generate_prompt_tmpl.format(\n",
    "            context_str=text, num_questions_per_chunk=num_questions_per_chunk\n",
    "        )\n",
    "    ...\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import openparse\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "import google.generativeai as genai\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from transformers import BitsAndBytesConfig\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.finetuning import generate_qa_embedding_pairs\n",
    "from llama_index.core.llms.chatml_utils import messages_to_prompt, completion_to_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_CACHE_DIR = \"../models/hf\"\n",
    "os.environ['HF_HOME'] = HF_CACHE_DIR\n",
    "\n",
    "TIKTOKEN_CACHE_DIR = \"../models/tiktoken\"\n",
    "os.environ[\"TIKTOKEN_CACHE_DIR\"] = TIKTOKEN_CACHE_DIR\n",
    "# assert os.path.exists(os.path.join(TIKTOKEN_CACHE_DIR, \"9b5ad71b2ce5302211f9c61530b329a4922fc6a4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read API tokens (SHOULD BE CREATED BY USER)\n",
    "with open('../reqs/tokens.json', 'r') as file:\n",
    "    tokens = json.load(file)\n",
    "\n",
    "HF_ACCESS_TOKEN = tokens['HF_ACCESS_TOKEN'][0]\n",
    "GOOGLE_API_KEY = tokens['GOOGLE_API_KEY'][0]\n",
    "\n",
    "login(token=HF_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set/Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API model\n",
    "llm = Gemini(\n",
    "    api_key = GOOGLE_API_KEY,\n",
    "    model = \"models/gemini-1.0-pro\",\n",
    "    temperature = 0.3,\n",
    ")\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Local model\n",
    "# llm_name = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit = True,\n",
    "#     bnb_4bit_compute_dtype = torch.float16,\n",
    "#     bnb_4bit_quant_type = \"nf4\",\n",
    "#     bnb_4bit_use_double_quant = True,\n",
    "# )\n",
    "\n",
    "# llm = HuggingFaceLLM(\n",
    "#     model_name = llm_name,\n",
    "#     tokenizer_name = llm_name,\n",
    "#     context_window = 2048,\n",
    "#     max_new_tokens = 512,\n",
    "\n",
    "#     generate_kwargs = {\n",
    "#         \"do_sample\": True,\n",
    "#         \"temperature\": 0.5,\n",
    "#     },\n",
    "#     model_kwargs = {\n",
    "#         # \"torch_dtype\": torch.float16,\n",
    "#         \"quantization_config\": quantization_config,\n",
    "#         \"cache_dir\": HF_CACHE_DIR,\n",
    "#     },\n",
    "#     device_map = \"auto\",\n",
    "#     is_chat_model = True,\n",
    "\n",
    "#     completion_to_prompt = completion_to_prompt,\n",
    "#     messages_to_prompt = messages_to_prompt,\n",
    "# )\n",
    "\n",
    "# Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_folders = 5\n",
    "train_files = []\n",
    "\n",
    "for i in range(num_train_folders):\n",
    "    train_files.append(glob.glob(f\"../data/finetune/docs/train_{i+1}/*.pdf\"))\n",
    "\n",
    "val_files = glob.glob(\"../data/finetune/docs/val/*.pdf\")\n",
    "\n",
    "train_files, val_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_corpus(files):\n",
    "    parser = openparse.DocumentParser(\n",
    "        table_args = {\"parsing_algorithm\": \"pymupdf\",},\n",
    "    )\n",
    "\n",
    "    nodes = []\n",
    "    for file in tqdm(files):\n",
    "        try:\n",
    "            nodes += parser.parse(file, ocr=True).to_llama_index_nodes()\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return nodes\n",
    "\n",
    "# Parse and save\n",
    "for i in range(num_train_folders):\n",
    "    train_nodes = parse_corpus(train_files[i])\n",
    "    with open(f'../data/finetune/docs/train_{i+1}/nodes.pkl', 'wb') as file: pickle.dump(train_nodes, file)\n",
    "    print(len(train_nodes))\n",
    "    \n",
    "val_nodes = parse_corpus(val_files)\n",
    "with open('../data/finetune/docs/val/nodes.pkl', 'wb') as file: pickle.dump(val_nodes, file) \n",
    "print(len(val_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_train_folders):\n",
    "    if i+1 == 5:\n",
    "        with open(f'../data/finetune/docs/train_{i+1}/nodes.pkl', 'rb') as file:\n",
    "            train_nodes = pickle.load(file)\n",
    "\n",
    "        train_dataset = generate_qa_embedding_pairs(\n",
    "            llm = llm,\n",
    "            nodes = train_nodes,\n",
    "            num_questions_per_chunk = 2,\n",
    "            output_path = f\"../data/finetune/datasets/train_{i+1}.json\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = generate_qa_embedding_pairs(\n",
    "    llm = llm,\n",
    "    nodes = val_nodes,\n",
    "    num_questions_per_chunk = 2,\n",
    "    output_path = f\"../data/finetune/datasets/val.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "from llama_index.finetuning import SentenceTransformersFinetuneEngine\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "\n",
    "\n",
    "stransformers_cache_dir = \"../models/stransformers\"\n",
    "os.environ[\"SENTENCE_TRANSFORMERS_HOME\"] = stransformers_cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmbeddingQAFinetuneDataset.from_json(\"../data/finetune/datasets/train_1.json\")\n",
    "val_dataset = EmbeddingQAFinetuneDataset.from_json(\"../data/finetune/datasets/val.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_name = \"dunzhang/stella_en_400M_v5\"\n",
    "embedding_name = \"Snowflake/snowflake-arctic-embed-l\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_engine = SentenceTransformersFinetuneEngine(\n",
    "    dataset = train_dataset,\n",
    "    model_id = embedding_name,\n",
    "    model_output_path = f\"../models/stransformers/{embedding_name.split('/')[1]}-finetuned\",\n",
    "    val_dataset = val_dataset,\n",
    "    batch_size = 5,\n",
    "    epochs = 5,\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "\n",
    "finetune_engine.finetune()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset, model_id, name):\n",
    "    output_path = \"../results/finetune\"\n",
    "    Path(output_path).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    corpus = dataset.corpus\n",
    "    queries = dataset.queries\n",
    "    relevant_docs = dataset.relevant_docs\n",
    "    evaluator = InformationRetrievalEvaluator(\n",
    "        queries, corpus, relevant_docs, name=name,\n",
    "    )\n",
    "\n",
    "    model = SentenceTransformer(model_id)\n",
    "\n",
    "    return evaluator(model, output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_base = evaluate(val_dataset, embedding_name, name=\"base\")\n",
    "results_finetuned = evaluate(val_dataset, f\"../models/stransformers/{embedding_name.split('/')[1]}-finetuned\", name=\"finetuned\")\n",
    "\n",
    "improvement = round((results_finetuned - results_base) / results_base * 100)\n",
    "\n",
    "print(results_base, results_finetuned)\n",
    "print(f\"Improvement: {improvement}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
